{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python version\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libflac8 libogg0 libvorbis0a libvorbisenc2\n",
      "The following NEW packages will be installed:\n",
      "  libflac8 libogg0 libsndfile1 libvorbis0a libvorbisenc2\n",
      "0 upgraded, 5 newly installed, 0 to remove and 17 not upgraded.\n",
      "Need to get 557 kB of archives.\n",
      "After this operation, 2051 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libogg0 amd64 1.3.2-1 [17.2 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libflac8 amd64 1.3.2-1 [213 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libvorbis0a amd64 1.3.5-4.2 [86.4 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic/main amd64 libvorbisenc2 amd64 1.3.5-4.2 [70.7 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsndfile1 amd64 1.0.28-4ubuntu0.18.04.2 [170 kB]\n",
      "Fetched 557 kB in 5s (114 kB/s)     \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libogg0:amd64.\n",
      "(Reading database ... 12700 files and directories currently installed.)\n",
      "Preparing to unpack .../libogg0_1.3.2-1_amd64.deb ...\n",
      "Unpacking libogg0:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libflac8:amd64.\n",
      "Preparing to unpack .../libflac8_1.3.2-1_amd64.deb ...\n",
      "Unpacking libflac8:amd64 (1.3.2-1) ...\n",
      "Selecting previously unselected package libvorbis0a:amd64.\n",
      "Preparing to unpack .../libvorbis0a_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbis0a:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libvorbisenc2:amd64.\n",
      "Preparing to unpack .../libvorbisenc2_1.3.5-4.2_amd64.deb ...\n",
      "Unpacking libvorbisenc2:amd64 (1.3.5-4.2) ...\n",
      "Selecting previously unselected package libsndfile1:amd64.\n",
      "Preparing to unpack .../libsndfile1_1.0.28-4ubuntu0.18.04.2_amd64.deb ...\n",
      "Unpacking libsndfile1:amd64 (1.0.28-4ubuntu0.18.04.2) ...\n",
      "Setting up libogg0:amd64 (1.3.2-1) ...\n",
      "Setting up libvorbis0a:amd64 (1.3.5-4.2) ...\n",
      "Setting up libflac8:amd64 (1.3.2-1) ...\n",
      "Setting up libvorbisenc2:amd64 (1.3.5-4.2) ...\n",
      "Setting up libsndfile1:amd64 (1.0.28-4ubuntu0.18.04.2) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y libsndfile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastai in /opt/conda/lib/python3.7/site-packages (2.7.9)\n",
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.7/site-packages (1.5.12)\n",
      "Collecting soundfile\n",
      "  Using cached soundfile-0.11.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastai) (21.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastai) (21.3)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai) (1.0.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai) (6.0)\n",
      "Requirement already satisfied: spacy<4 in /opt/conda/lib/python3.7/site-packages (from fastai) (3.4.1)\n",
      "Requirement already satisfied: torch<1.14,>=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai) (1.0.2)\n",
      "Requirement already satisfied: torchvision>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai) (0.13.0)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai) (0.0.7)\n",
      "Requirement already satisfied: pillow>6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai) (9.0.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastai) (1.3.5)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai) (1.7.3)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai) (3.5.3)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai) (1.5.27)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.63.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2022.6.15)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.26.8)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle) (6.1.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.7/site-packages (from soundfile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (8.1.3)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (4.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (2.4.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (61.2.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (3.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (1.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (1.21.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (0.6.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (1.9.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy<4->fastai) (0.10.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from catalogue<2.1.0,>=2.0.6->spacy<4->fastai) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->fastai) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy>=0.3.5->spacy<4->fastai) (5.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fastai) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->fastai) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.7.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.7/site-packages (from thinc<8.2.0,>=8.1.0->spacy<4->fastai) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer<0.5.0,>=0.3.0->spacy<4->fastai) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click<9.0.0,>=7.1.1->typer<0.5.0,>=0.3.0->spacy<4->fastai) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->spacy<4->fastai) (2.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai) (4.37.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->fastai) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->fastai) (2022.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->fastai) (3.1.0)\n",
      "Installing collected packages: soundfile\n",
      "Successfully installed soundfile-0.11.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install fastai\n",
    "!pip3 install fastai kaggle soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Kaggle’s public API, you must first authenticate using an API token. From the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings at https://www.kaggle.com/account. Scroll down to the section of the page labelled API:\n",
    "\n",
    "To create a new token, click on the “Create New API Token” button. This will download a fresh authentication token onto your machine.\n",
    "\n",
    "### Accept the rules\n",
    "\n",
    "https://www.kaggle.com/competitions/whale-detection-challenge/rules\n",
    "\n",
    "\n",
    "### Upload your `kaggle.json` to the same folder as this notebook then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/kaggle; mv kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c whale-detection-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf full_data; rm -rf sample_data; rm -rf tmp_data; #remove any existing extracted data\n",
    "!unzip -q whale-detection-challenge.zip -d data/ #unzip main file\n",
    "!unzip -q data/small_data_sample_revised.zip -d sample_data/ #unzip sample data\n",
    "!unzip -q data/whale_data.zip -d tmp_data/ #unzip full data\n",
    "!rm -rf data/; rm -rf tmp_data/data/test; #remove unneeded files. official test data isn't used because we don't have labels\n",
    "!mkdir full_data; mv tmp_data/data/train full_data/audio; #move stuff around\n",
    "!mv tmp_data/data/train.csv full_data/labels.csv #rename labels\n",
    "!rm -rf tmp_data #remove tmp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import * #used for fastai\n",
    "from IPython import display #used to display media in notebook\n",
    "import soundfile as sf #used to load sound files\n",
    "import matplotlib.pyplot as plt #used to plot in notebook\n",
    "import scipy\n",
    "from scipy.signal import hann\n",
    "from scipy.fftpack import rfft\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the metadata file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio loading based on: https://github.com/limazix/audio-processing/blob/master/audio_classifier_fastai.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'full_data'))\n",
    "DATA_META_FILE=os.path.join(DATA_ROOT_DIR, 'labels.csv')\n",
    "DATA_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'audio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_name    train1.aiff\n",
      "label                  0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data_meta = pd.read_csv(DATA_META_FILE)\n",
    "print(data_meta.iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_name'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1195/3152588424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_meta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1195/3152588424.py\u001b[0m in \u001b[0;36mbuild_file_path\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_file_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_name'"
     ]
    }
   ],
   "source": [
    "def build_file_path(item):\n",
    "    item['path'] = item['file_name']\n",
    "    item['label'] = item['label']\n",
    "    return item[['path', 'label']]\n",
    "\n",
    "data = data_meta.apply(build_file_path, axis=1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train1.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train2.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train3.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train4.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train5.aiff</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     clip_name  label\n",
       "0  train1.aiff      0\n",
       "1  train2.aiff      0\n",
       "2  train3.aiff      0\n",
       "3  train4.aiff      0\n",
       "4  train5.aiff      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./full_data/labels.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders_from_csv(\n",
    "  \"./full_data/audio\",\n",
    "  csv_fname = \"labels.csv\",\n",
    "  header = \"infer\",\n",
    "  delimiter = \",\",\n",
    "  valid_pct = 0.2,\n",
    "  seed = 42,\n",
    "  fn_col = 0,\n",
    "  label_col = 1,\n",
    ")\n",
    "path = Path(\"./audio\")\n",
    "cfg = AudioConfig.BasicMelSpectrogram(n_fft=512)\n",
    "a2s = AudioToSpec.from_cfg(cfg)\n",
    "auds = DataBlock(blocks = (AudioBlock, CategoryBlock),  \n",
    "                 get_x = ColReader(\"clip_name\", pref=path), \n",
    "                 valid_pct = 0.2,\n",
    "                 batch_tfms = [a2s],\n",
    "                 get_y = ColReader(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The country depends on the shipping industry's ability to transport goods across large distances. The North Atlantic ocean houses a large portion of the nation's shipping industry as well as the last remaining 350 North Atlantic Right whales. Right whales die in large part due to collisions with commercial ships. Studies estimate that saving two female right whales per year could put their population back on track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioacoustics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bioacoustics is the science combining biology and acoustics. In this particular domain we use bioacoustics to investigate sound as a tool to detect the presence of North Atlantic right whales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does a North Atlantic Right Whale sound like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "North Atlantic right whales can make many sounds. The most distinctive of which is called the up-call. The North Atlantic right whale uses this call to tell other right whales that it is nearby. It can be thought of as a 'contact call'. It is a low frequency 'whoop' sound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To play an audio file in Python, we first have to load that audio file into memory. As we'll see shortly, an audio file is little more than a sequence of numbers. Here's some sample Python that loads in whale call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_audio, whale_samplerate = sf.read(\"./data_sample/right_whale/train6.aiff\")\n",
    "display.Audio(whale_audio, rate=samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Introduction to Signal Processing (Using Whale Sounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale_audio, whale_samplerate = sf.read(\"./data_sample/right_whale/train6.aiff\")\n",
    "not_whale_audio, not_whale_samplerate = sf.read(\"./data_sample/no_right_whale/train1.aiff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampling rate refers to the number of samples of audio recorded every second. It is measured in samples per second or Hertz (abbreviated as Hz or kHz, with one kHz being 1000 Hz). \n",
    "\n",
    "The sampling rate is analogous to the frame rate or FPS (frames per second) measurement for videos. A video is simply a series of pictures, usually called in this context “frames”, displayed back to back very quickly to give the illusion (at least to us humans) of continuous non-interrupted motion or movement.\n",
    "\n",
    "Source: https://www.vocitec.com/docs-tools/blog/sampling-rates-sample-depths-and-bit-rates-basic-audio-concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Raw Signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take a look at the plot of the raw signal of the audio file. Unfortunately, this representation does not provide us with very much information to analyze the contents of the audio file. The plot provides us with a representation of the signal in the time dimension as it relates to amplitude but it is not very intuitive. When we hear sound, our ears interpret pitch. Pitch is a term to describe our understanding of frequency. As humans, it may be more meaningful for us to extract to frequency content of the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(whale_audio)\n",
    "# label the axes\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "# set the title\n",
    "plt.title(\"Whale Sample\")\n",
    "# display the plot\n",
    "plt.show()\n",
    "\n",
    "plt.plot(not_whale_audio)\n",
    "# label the axes\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.xlabel(\"Time (samples)\")\n",
    "# set the title\n",
    "plt.title(\"Not Whale Sample\")\n",
    "# display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we have a total of 4000 samples since each recording has a sample rate of 2000Hz and a total length of 2 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets zoom in a bit to just the first hundred samples to take a closer look a small section of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(audio, title):\n",
    "  plt.plot(audio)\n",
    "  # label the axes\n",
    "  plt.ylabel(\"Amplitude\")\n",
    "  plt.xlabel(\"Time (samples)\")\n",
    "  # set the title\n",
    "  plt.title(title)\n",
    "  # display the plot\n",
    "  plt.xlim(0, 100)\n",
    "  plt.show()\n",
    "\n",
    "plot_signal(whale_audio, \"Whale Sample Signal\")\n",
    "plot_signal(whale_audio, \"Not Whale Sample Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a clearer picture of what is going on in the audio file, it is useful to examine the frequency content of the signal. To do this we will breakdown the original signal into its frequency components and plot it. Here we perform a Fast Fourier Transform on the signal and plot it. Now, we get a clearer picture of which frequencies are used in the sound file as well as their amplitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frequency(audio, title):\n",
    "  # apply a Hanning window\n",
    "  window = hann(4000)\n",
    "  audio = audio * window\n",
    "  # fft\n",
    "  mags = abs(rfft(audio))\n",
    "  # convert to dB\n",
    "  mags = 20 * scipy.log10(mags)\n",
    "  # normalise to 0 dB max\n",
    "  mags -= max(mags)\n",
    "  # plot\n",
    "  plt.plot(mags)\n",
    "  # label the axes\n",
    "  plt.ylabel(\"Magnitude (dB)\")\n",
    "  plt.xlabel(\"Frequency Bin\")\n",
    "  # set the title\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "plot_frequency(whale_audio, \"Whale Sound Frequency Analysis\")\n",
    "plot_frequency(not_whale_audio, \"Not Whale Sound Frequency Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Frequency Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be really useful would be to see which frequencies are active, and at what intensity, at a given point in time? Here we produce a spectrogram which gives us an image representing a breakdown of frequencies plotted against time. A dark color in the plot represents intense activity at that frequency at that time. This gives us a much more helpful picture where we can see in the middle the call of the whale going upwards in pitch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_spectrogram(audio, title):\n",
    "  fs = 2000 # sampling frequency\n",
    "\n",
    "  # Plot the spectrogram\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  S, freqs, bins, im = plt.specgram(audio, NFFT=256, Fs=fs, noverlap=192)\n",
    "  plt.xlabel('Time')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.title(title)\n",
    "  plt.show()\n",
    "\n",
    "show_spectrogram(whale_audio, \"Whale Audio Spectrogram\")\n",
    "show_spectrogram(not_whale_audio, \"Not Whale Audio Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cornell Lab of Ornithology Buoy System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cornell Lab of Ornithology used a bouy platform from the Woods Hole Oceanographic Institution. The WHOI platform components included mooring, a hydrophone, a power system, surface expression, and antennae. They also included Cornell components such as housing, analog interface hardware, GPS, an embedded computer, a detection engine, and telemetry hardware. The bouy hardware/software system would rank 2s sound clips as potential North Atlantic Right Whale candidates and would upload the clip along with meta data such as location, timestamp, and more via a satellite for shore-side processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_spectrogram(whale_audio, \"Whale Audio Spectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the image above as the 'image' of what a NARW (North Atlantic Right Whale) call looks like. This image is called a spectogram and what it shows is the activity of a signal in the frequency domain plotted against time. Notice the brighter spot in the 0.5 - 1 second time range. That represents a NARW up-call as it goes up in frequency over time. The more intense the pixels in a region, the more activity in that corresponding frequency is going on at that particular time. We notice that a whale call has an upward trajectory in frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Detection Methods 2005-2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input from a hydrophone (underwater microphone) is input into an analog to digital converter. The magnitude-squared discrete fourier transform spectogram is computed from the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to detect a NARW up-call the spectrogram is processed by Doug Gillespie's NARW upsweep detector. Essentially, an estimate for the background level of noise is formed. The detector would check from time-frequency cells with energy greater than an integer multiple of the background energy estimate. Cells with energy over this level were grouped in time and frequency (called candidate events). A numerical score from 2-10 was given to each event based on the duration of the event, min/max frequency, sweep frequency, position of min/max Hz, and max bandwidth. If the score of the event exceeded a threshold then its time and sore were stored. Low scoring events were constantly being replaced by better scoring events. After all of this only the 10 best scoring clips were to be stored in long term bouy memory. These clips would then be uploaded shore-side where experts manually annotate whether or not a whale was present in the clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, The Cornell Bioacoustics Research Program and Marinexplore held a public data science contest to come up with new solutions to NARW detection. A variety of different approaches were used by different teams. The benchmark score to beat was 0.7 ROC AUC. The method to get that score was based on the existing system in place today by Cornell, outlined above. The winning solution managed to get a score of 0.981 ROC AUC which is a significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Winning Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winning approach meets the objective by finding an automatic way of detection NARW up-calls without the need for experts to have to sit through and manually label incoming events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at a large number of images of spectrograms of whale calls, patterns start to emerge. Most whale calls exist in a frequency range of 50hz-400hz. Most whale calls also include an upword trajectory in pitch. Differences exist in the slope of the trajectory and the length of the call. The winning approach uses the notion that most whale calls \"look\" alike, when looked at in a spectrogram, and tries to use that to it's advantage. Rather than pass an entire image (of the spectrogram) to a classification algorithm, they pass a small list of features derived from template matching (and other metrics). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template matching finds areas of an image that are similar to a template image. The template image is often called a patch. \n",
    "\n",
    "Two components are needed:\n",
    "* A Source image\n",
    "* Template image\n",
    "    \n",
    "The goal is to detect the highest matching area of a source image.\n",
    "\n",
    "To find a matching area, you overlay the template image over the source image and compare the areas. The comparison is given a score and is stored in a result matrix R at the index (x,y) corresponding to the current rectangle formed with top left corner at location (x,y) in the source image. After the comparison is made, the method then slides the template over one pixel and compares against that rectangle. This process continues for all pixels in the source image.\n",
    "\n",
    "A score is then calculated from this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How were templates generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a training sample did not correlate to any template well, then that training sample was converted to a template. This process was repeated until most training samples matched templates. This process was not automatic and was done manually. The template file read in by this method was merely a filename of an image, an x0 and xN positions (start and finish, x-axis), and a y0 and yN positions (start and finish, y-axis). Together these positions represent a rectangle inside a given image file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre-Processing the Spectrogram Before Template Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using a given rectangle of an image as a template, they did a few pre-processing steps.\n",
    "\n",
    "First was to demean the image.\n",
    "\n",
    "What this does is to cut off extreme values and remove the mean at a given pixel. This enhances the contrast vertically along the frequency dimension.\n",
    "\n",
    "The next step was to normalize the image.\n",
    "\n",
    "Afer that, the image was turned into a binary mask. Pixels were rounded to 1 if the intensity was greater than 0.5 and to 0 if less than 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A step-by-step example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, freqs, bins, im = plt.specgram(whale_audio, NFFT=256, Fs=whale_samplerate, noverlap=192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertically Demean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slidingWindowV(P,inner=3,outer=64,maxM = 50,norm=True):\n",
    "\tQ = P.copy()\n",
    "\tm, n = Q.shape\n",
    "\tif norm:\n",
    "\t\tmval, sval = np.mean(Q[:maxM,:]), np.std(Q[:maxM,:])\n",
    "\t\tfact_ = 1.5\n",
    "\t\tQ[Q > mval + fact_*sval] = mval + fact_*sval\n",
    "\t\tQ[Q < mval - fact_*sval] = mval - fact_*sval\n",
    "\twInner = np.ones(inner)\n",
    "\twOuter = np.ones(outer)\n",
    "\tfor i in range(n):\n",
    "\t\tQ[:,i] = Q[:,i] - (np.convolve(Q[:,i],wOuter,'same') - np.convolve(Q[:,i],wInner,'same'))/(outer - inner)\n",
    "\treturn Q[:maxM,:]\n",
    "\n",
    "m, n = P.shape\n",
    "P = slidingWindowV(P,maxM=m)\n",
    "plot_specgram(P, freqs, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize and Turn into Binary Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpl = P.astype('Float32')\n",
    "m_, s_ = np.mean(tmpl), np.std(tmpl)\n",
    "min_ = tmpl.min()\n",
    "tmpl[ tmpl < m_ + 0.5*s_] = min_ \n",
    "tmpl[ tmpl > min_ ] = 1\n",
    "tmpl[tmpl < 0] = 0\n",
    "\n",
    "plot_specgram(tmpl, freqs, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cut out a Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=12\n",
    "xn=26\n",
    "y0=23\n",
    "yn=50\n",
    "\n",
    "tmpl = P[x0:xn,y0:yn].astype('Float32')\n",
    "m_, s_ = np.mean(tmpl), np.std(tmpl)\n",
    "min_ = tmpl.min()\n",
    "tmpl[ tmpl < m_ + 0.5*s_] = min_ \n",
    "tmpl[ tmpl > min_ ] = 1\n",
    "tmpl[tmpl < 0] = 0\n",
    "\n",
    "Z = np.flipud(tmpl)\n",
    "\t\t\n",
    "extent = x0, xn, y0, yn\n",
    "figure()\n",
    "im = imshow(Z, extent=extent)\n",
    "axis('auto')\n",
    "xlim([x0, xn])\n",
    "ylim([y0, yn])\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating a Feature from Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When template matching is called on a source image and template, the result is the (x,y) coordinate with the maximum correlation to the template as well as the score it obtained. There are many different scoring formulas but the one used in this solution is the equation mentioned above. This process is repeated for every template. Therefore what they end up with is a list of how well an image correlates to a template, for every template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Frequency Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other metrics used as features were the centroid, width, skew, and total variation of a time slice of frequencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this feature generation, the results are sent to a Gradient Boosting Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is a machine learning tecnique for regression problems. It produces a prediction model in the form of an ensemble of weak prediction models. Gradient boosting predictors are built in a stage-wise fashion. Gradient boosting is used for classification by reducing the classification task to a regression problem with a suitable loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method outline here received a 0.981 ROC AUC score. It received an even higher score when temporal information in the file naming was used as a feature. Files in the dataset were numbered sequentially, and incidently were also chronologically ordered. What ended up happening was that clusters of whale calls would appear in proximity to each other temporaly, represented as filenames that were close to each other. By utilizing this information, the score jumped to 0.9838."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area under the Receiver Operating Characteristic Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wikipedia - Receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC is the area of the region underneath this curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the ROC Curve is the sensitivity (true positives rate) plotted against false positives rate (1-specificity) as the discrimination threshold is varied.\n",
    "\n",
    "True positive rate is also called sensitiviy, recall, or hit-rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "tprate=\\frac{\\text{Positives correctly classified}}{\\text{Total positives}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false positive rate (also called false alarm rate) of the\n",
    "classifier is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "fprate=\\frac{\\text{Negatives incorrectly classified}}{\\text{Total negatives}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Convolutional Neural Network Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are models inspired by animal central nervous systems (in particular the brain) that are capable of machine learning and pattern recognition. The first generation of neural networks, called perceptrons, were introduced in 1957 by Frank Rosenblatt. They were deemed ineffective by the artificial intelligence community. Recently, neural networks have made a comeback as we now have more tools to properly train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNN) are variants of Multi-layer Perceptrons. They are modeled after Hubel and Wisel's early work on the cat's visual cortex. They are designed to recognize visual patterns directly from pixel images with minimal preprocessing. They can recognize patterns with extreme variability, and with robustness to distortions and simple geometric transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Preparing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN take images as input, therefore it is natural that we pass it our mean spectrograms as input. We first scale down our mean spectrograms to 64x29 px images with 3 channels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Storing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store our data in HDF5 datasets using the H5PY library in Python. This allows us to store our data on disk, instead of RAM, which is good for large datasets. Pylearn2, requires data in the form of an object of type DenseDesignMatrix which provides special handlers for dealing with the dataset. A simple wrapper is created to wrap the HDF5 datasets as a DenseDesignMatrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two hidden layers to the neural network. Both contain a convolution layer and a subsampling layer in the form of a single ConvRectifiedLinear layer. These layers make the neural network invariant to small changes such as translation or scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Soft-Max Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Soft-Max layer calculates the output of the neural network from the input of the layers above. Essentially, it performs logistic regression after applying weights to the input and gives out a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Training the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of one layer depends on another, we make a full pass over the network and calculate the derivative of the error. We use stochastic gradient descent while backpropagating the derivative of the error to the previous layers to adjust the weights of each unit in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
