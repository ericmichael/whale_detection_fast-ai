{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: Fill out this notebook to import your trained model and build a Gradio interface.\n",
    "\n",
    "After mocking up Gradio interface: Deploy your model to HuggingFace Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-3.4.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from gradio) (1.21.5)\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from gradio) (1.8.2)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 28.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 116.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from gradio) (1.3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from gradio) (6.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from gradio) (3.5.2)\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "\u001b[K     |████████████████████████████████| 948 kB 91.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting orjson\n",
      "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 102.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from gradio) (2.27.1)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.85.0-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 18.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx\n",
      "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 21.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting paramiko\n",
      "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 90.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from gradio) (9.0.1)\n",
      "Collecting websockets\n",
      "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 108.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11<0.13,>=0.11\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 20.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
      "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 22.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pycryptodome\n",
      "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.3 MB 70.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-multipart\n",
      "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gradio) (22.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 20.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->gradio) (3.10.0.2)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[K     |████████████████████████████████| 148 kB 124.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[K     |████████████████████████████████| 231 kB 110.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (3.3)\n",
      "Collecting starlette==0.20.4\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 15.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.7/site-packages (from starlette==0.20.4->fastapi->gradio) (3.6.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.7/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi->gradio) (1.3.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from httpx->gradio) (2022.9.24)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting httpcore<0.16.0,>=0.15.0\n",
      "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
      "\u001b[K     |████████████████████████████████| 68 kB 28.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->gradio) (2.0.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mdit-py-plugins\n",
      "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 23.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting linkify-it-py~=1.0\n",
      "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->gradio) (1.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->gradio) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->gradio) (2022.1)\n",
      "Collecting pynacl>=1.0.1\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[K     |████████████████████████████████| 856 kB 91.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bcrypt>=3.1.3\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
      "\u001b[K     |████████████████████████████████| 593 kB 82.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /opt/conda/lib/python3.7/site-packages (from paramiko->gradio) (37.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=2.5->paramiko->gradio) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->gradio) (1.26.8)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from uvicorn->gradio) (8.1.3)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click>=7.0->uvicorn->gradio) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click>=7.0->uvicorn->gradio) (3.9.0)\n",
      "Building wheels for collected packages: ffmpy, python-multipart\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=2336d524e75173c5818c4db9a4b784932aa3cd7a0debe9f01fe30b80b55da058\n",
      "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
      "  Building wheel for python-multipart (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=154a2a1a0e02fb7aa1356ab360dbbbc56ffd76d6b196a34705b2af61ea0f18fa\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
      "Successfully built ffmpy python-multipart\n",
      "Installing collected packages: mdurl, uc-micro-py, rfc3986, multidict, markdown-it-py, h11, frozenlist, yarl, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, bcrypt, asynctest, async-timeout, aiosignal, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, fsspec, ffmpy, fastapi, aiohttp, gradio\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 bcrypt-4.0.1 fastapi-0.85.0 ffmpy-0.3.0 frozenlist-1.3.1 fsspec-2022.8.2 gradio-3.4.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 multidict-6.0.2 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.18.3 websockets-10.3 yarl-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import * #used for fastai\n",
    "from IPython import display #used to display media in notebook\n",
    "import matplotlib.pyplot as plt #used to plot in notebook\n",
    "\n",
    "from fastai.data.all import *\n",
    "from fastai.data.external import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import librosa;\n",
    "import librosa.display;\n",
    "\n",
    "import PIL as Pillow;\n",
    "import gradio as gr;\n",
    "import soundfile as sf;\n",
    "import tempfile;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the learner from `.pkl` file. \n",
    "\n",
    "This will complain that you need some functions that aren't available in the namespace. Copy the necessary ones from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: put the functions that are missing from the namespace here\n",
    "# ....\n",
    "# This code takes a single channel image (greyscale) and converts it into a 3-channel image (RGB)\n",
    "# It also normalizes so that all values are between [0,255]\n",
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Scale to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "# Lots of libraries and methods for generating a spectrogram\n",
    "# Under the hood all these algorithms rely on a Fast Fourier Transform\n",
    "# Originally, I wanted to use torchaudio because that is CUDA enabled and can be accelerated on GPU\n",
    "# Unfortunately, the shape of the data returned simply did not make any sense and working with tensors\n",
    "# as opposed to numpy arrays was incredibly annoying. It just didn't work.\n",
    "# I tried other libraries as well but ultimately settled on librosa since it seems the most widely used\n",
    "# There exist fast.ai packages like fastaudio and other forks but like many open-source things they are unmaintained\n",
    "# Using those type of packages breaks the dependencies and on my environment forced a non-CUDA\n",
    "# accelerated version of pytorch which is useless\n",
    "def create_spectrogram(file_path):\n",
    "    samples, sample_rate = librosa.core.load(file_path, sr=2000)\n",
    "    \n",
    "    # the parameters here are tunable and are hard-coded to what i've found works well for this dataset\n",
    "    n_fft=256\n",
    "    hop_length=32\n",
    "    win_length=192\n",
    "    \n",
    "    # Compute spectrogram, using some sensible defaults\n",
    "    # Opportunities here to tweak possibly, just not sure how much it would help\n",
    "    # We don't use Nicholas' settings as for some reason they get bad results with this library\n",
    "    D = librosa.stft(samples, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    \n",
    "    # Normalize to decibals\n",
    "    S_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    return S_dB, sample_rate\n",
    "\n",
    "# This is an alternative type of spectrogram.\n",
    "# My understanding is that it normalizes the spectrogram based on what humans can perceive which is\n",
    "# Settings here are open to tweaking\n",
    "def create_mel_spectrogram(file_path):\n",
    "    samples, sample_rate = librosa.core.load(file_path, sr=2000)\n",
    "    \n",
    "    # the parameters here are tunable and are hard-coded to what i've found works well for this dataset\n",
    "    n_fft=256\n",
    "    hop_length=32\n",
    "    win_length=192\n",
    "    fmax = 1000 # cut off at 1000Hz\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=samples, sr=sample_rate, fmax=fmax, n_fft=n_fft, hop_length=hop_length, win_length=win_length, center=False)\n",
    "    \n",
    "    # Normalize to decibals\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB, sample_rate\n",
    "\n",
    "\n",
    "# Converts a spectrogram (numpy) to a 3-channel (RGB) Fast.AI Image\n",
    "def spectrogram_to_image(spec):\n",
    "    \n",
    "    # Most vision models in fast.ai use images with three channels (RGB)\n",
    "    # spectrogram functions don't return images, they return data\n",
    "    # plot libraries like matplotlib help us visualize the data as an image, but it is not an image\n",
    "    # it is a multi-dimensional array-like object whose values can be positive or negative\n",
    "    \n",
    "    # We need to convert it from this format into a 3 channel (RGB) whose values are bounded between [0, 255]\n",
    "    colored_np = mono_to_color(spec)\n",
    "    \n",
    "    # Pillow is a fork of PIL (standard Python image library), we consider Pillow.Image to be regular Python images\n",
    "    # In order to use Pillow features like crop, we have to convert the image from numpy into PIL (Pillow)\n",
    "    pillow_image = Pillow.Image.fromarray(colored_np) # convert to regular python image\n",
    "    \n",
    "    # When you manually convert a spectrogram to an image without using matplotlib you have to flip it vertically\n",
    "    flipped_image = pillow_image.transpose(Image.FLIP_TOP_BOTTOM) # flip image\n",
    "    \n",
    "    # This will crop the image by taking from the height to make a square\n",
    "    h, w, *other = pillow_image.shape\n",
    "    cropped_image = flipped_image.crop((0, h-w, w, h))\n",
    "    cropped_image_np = np.array(cropped_image) # back to numpy\n",
    "        \n",
    "    # Kinda confusing but fast.ai has a class called PILImage and so we convert our real PIL image into a fast.ai one\n",
    "    fast_ai_image = PILImage.create(cropped_image_np)\n",
    "    return fast_ai_image\n",
    "\n",
    "\n",
    "# I chose to load the audio files directly into fast.ai using the DataBlock API.\n",
    "# Alternatively, we could have pre-computed all the spectrograms in the 00_getting_started.ipynb but\n",
    "# I decided against it because then it would be unlikely for anyone to actually make modifications to the images\n",
    "# Writing 40,000 files to disk is painfully slow so you wouldn't even be able to get started quickly\n",
    "# This method creates a transformer which can take paths to audio files and transform them into spectrogram images.\n",
    "# If you do some research you will see lots of people doing pretty terrible things involving file.io because\n",
    "# it is not easy to figure out how to turn a spectrogram into a 3-channel image and further how to get that\n",
    "# into a fast.ai image. This solution is pretty clever because since it is all in-memory it is insanely fast and\n",
    "# does not rely on any type of pre-computation.\n",
    "class SpecgramTransform(Transform):\n",
    "    def __init__(self): self.aug = create_spectrogram\n",
    "    def encodes(self, audio_file: Path):\n",
    "        aug_img, sample_rate = self.aug(audio_file)\n",
    "        image = spectrogram_to_image(aug_img)\n",
    "        return image\n",
    "\n",
    "# Alternative transformer for generating images of Mel Spectrograms\n",
    "class MelSpecgramTransform(Transform):\n",
    "    def __init__(self): self.aug = create_mel_spectrogram\n",
    "    def encodes(self, audio_file: Path):\n",
    "        aug_img, sample_rate = self.aug(audio_file)\n",
    "        image = spectrogram_to_image(aug_img)\n",
    "        return image\n",
    "\n",
    "    \n",
    "# IGNORE: Unless you decide to use SpecgramTransform \n",
    "# We may need to crop images. If you use the SpecgramTransform, cropping will likely be required \n",
    "# to turn the image into a square.\n",
    "# It could be reasonable to cut-off the image at frequencies we know a whale call couldn't exist\n",
    "# Ultimately, images will need to be square I believe so they would need to get filled with something\n",
    "# The reason I created this transform is because fast.ai doesn't give you a transformer for precision cropping\n",
    "# You either crop and cut out the center or you don't crop at all\n",
    "# This transformer lets you optionally crop from any direction and leaves sides alone that you don't specify crops for\n",
    "class CropImageTransform(Transform):\n",
    "    def __init__(self, left=None, upper=None, right=None, lower=None):\n",
    "        print(\"test constructor\")\n",
    "        self.aug = self.__crop_image\n",
    "        self.box = (left, upper, right, lower)\n",
    "    \n",
    "    def __compute_box(self, image):\n",
    "        # get current dimensions of image\n",
    "        # *other is because we don't know if we will receive two elements or more\n",
    "        # we get two for a greyscale image, we get three for a RGB\n",
    "        h, w, *other = image.shape\n",
    "        \n",
    "        #get desired crop entered by user\n",
    "        left, upper, right, lower = self.box\n",
    "        \n",
    "        #don't crop sides that user didn't want cropped\n",
    "        left = 0 if left is None else left\n",
    "        upper = 0 if upper is None else upper\n",
    "        right = w if right is None else right\n",
    "        lower = h if lower is None else lower\n",
    "        \n",
    "        #save computed box\n",
    "        self.box = (left, upper, right, lower)\n",
    "        \n",
    "    def __crop_image(self, image):\n",
    "        print(\"test crop\")\n",
    "\n",
    "        self.__compute_box(image)\n",
    "        print(self.box)\n",
    "        image_cropped = image.crop(self.box)\n",
    "        return image_cropped\n",
    "        \n",
    "    # this transformer works on PILImages (fast.ai)\n",
    "    # this transfomer does not work on PIL.Image (Pillow/PIL)\n",
    "    def encodes(self, image):\n",
    "        print(\"encode\")\n",
    "        print(image)\n",
    "        cropped_image = self.aug(image)\n",
    "        return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define a prediction function for our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = load_learner('model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = learn.dls.vocab\n",
    "\n",
    "def predict(audio):\n",
    "    # grab data from Gradio upload\n",
    "    sample_rate, data = audio \n",
    "    \n",
    "    # recall that our dataset loads paths to audio files first, not the files themselves\n",
    "    # lets make a temporary (in-memory) file\n",
    "    temp_file = tempfile.NamedTemporaryFile(suffix='.aiff')\n",
    "    \n",
    "    # use soundfile library to write to temp file\n",
    "    sf.write(temp_file.name, data, sample_rate)\n",
    "    \n",
    "    # get our prediction results\n",
    "    pred,pred_idx,probs = learn.predict(Path(temp_file.name))\n",
    "    \n",
    "    # close tempfile\n",
    "    temp_file.close()\n",
    "    \n",
    "    # return prediction results\n",
    "    return {labels[i]: float(probs[i]) for i in range(len(labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n",
      "/opt/conda/lib/python3.7/site-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://20886.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://20886.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fd56e313f10>,\n",
       " 'http://127.0.0.1:7860/',\n",
       " 'https://20886.gradio.app')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "title = \"North Atlantic Right Whale Classifier\"\n",
    "description = \"A NARW up-call classifier trained on the The Marinexplore and Cornell University Whale Detection Challenge dataset (Kaggle) with fastai.\"\n",
    "article=\"<p style='text-align: center'><a href='https://www.kaggle.com/competitions/whale-detection-challenge' target='_blank'>Dataset</a></p>\"\n",
    "enable_queue=True\n",
    "\n",
    "gr.Interface(fn=predict, inputs=gr.Audio(type=\"numpy\"), outputs=gr.outputs.Label(num_top_classes=2),title=title,description=description,article=article,allow_flagging=\"never\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy to HuggingFace Spaces"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
