{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train additional models using fast.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some different ideas for improving the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import * #used for fastai\n",
    "from IPython import display #used to display media in notebook\n",
    "import matplotlib.pyplot as plt #used to plot in notebook\n",
    "\n",
    "from fastai.data.all import *\n",
    "from fastai.data.external import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import librosa;\n",
    "import librosa.display;\n",
    "\n",
    "import PIL as Pillow;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if CUDA is available - This will be a problem later if this doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'full_data'))\n",
    "SAMPLE_DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'sample_data'))\n",
    "\n",
    "DATA_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'whale')\n",
    "DATA_NOT_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'not_whale')\n",
    "path = Path(DATA_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Fast.AI Transformer to Load Audio to Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code takes a single channel image (greyscale) and converts it into a 3-channel image (RGB)\n",
    "# It also normalizes so that all values are between [0,255]\n",
    "def mono_to_color(X, mean=None, std=None, norm_max=None, norm_min=None, eps=1e-6):\n",
    "    # Stack X as [X,X,X]\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "    _min, _max = Xstd.min(), Xstd.max()\n",
    "    norm_max = norm_max or _max\n",
    "    norm_min = norm_min or _min\n",
    "    if (_max - _min) > eps:\n",
    "        # Scale to [0, 255]\n",
    "        V = Xstd\n",
    "        V[V < norm_min] = norm_min\n",
    "        V[V > norm_max] = norm_max\n",
    "        V = 255 * (V - norm_min) / (norm_max - norm_min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        # Just zero\n",
    "        V = np.zeros_like(Xstd, dtype=np.uint8)\n",
    "    return V\n",
    "\n",
    "# Lots of libraries and methods for generating a spectrogram\n",
    "# Under the hood all these algorithms rely on a Fast Fourier Transform\n",
    "# Originally, I wanted to use torchaudio because that is CUDA enabled and can be accelerated on GPU\n",
    "# Unfortunately, the shape of the data returned simply did not make any sense and working with tensors\n",
    "# as opposed to numpy arrays was incredibly annoying. It just didn't work.\n",
    "# I tried other libraries as well but ultimately settled on librosa since it seems the most widely used\n",
    "# There exist fast.ai packages like fastaudio and other forks but like many open-source things they are unmaintained\n",
    "# Using those type of packages breaks the dependencies and on my environment forced a non-CUDA\n",
    "# accelerated version of pytorch which is useless\n",
    "def create_spectrogram(file_path):\n",
    "    samples, sample_rate = librosa.core.load(file_path)\n",
    "    \n",
    "    # Compute spectrogram, using some sensible defaults\n",
    "    # Opportunities here to tweak possibly, just not sure how much it would help\n",
    "    # We don't use Nicholas' settings as for some reason they get bad results with this library\n",
    "    D = librosa.stft(samples, n_fft=512, hop_length=64, window=\"han\", center=False)  # STFT of y\n",
    "    \n",
    "    # Normalize to decibals\n",
    "    S_dB = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    return S_dB, sample_rate\n",
    "\n",
    "# This is an alternative type of spectrogram.\n",
    "# My understanding is that it normalizes the spectrogram based on what humans can perceive which is\n",
    "# Settings here are open to tweaking\n",
    "def create_mel_spectrogram(audio):\n",
    "    audio, sample_rate = librosa.core.load(audio)\n",
    "    # Compute mel spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=audio, sr=sample_rate, fmax=1000, center=False)\n",
    "    \n",
    "    # Normalize to decibals\n",
    "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_dB, sample_rate\n",
    "\n",
    "\n",
    "# Converts a spectrogram (numpy) to a 3-channel (RGB) Fast.AI Image\n",
    "def spectrogram_to_image(spec):\n",
    "    \n",
    "    # Most vision models in fast.ai use images with three channels (RGB)\n",
    "    # spectrogram functions don't return images, they return data\n",
    "    # plot libraries like matplotlib help us visualize the data as an image, but it is not an image\n",
    "    # it is a multi-dimensional array-like object whose values can be positive or negative\n",
    "    \n",
    "    # We need to convert it from this format into a 3 channel (RGB) whose values are bounded between [0, 255]\n",
    "    colored_np = mono_to_color(spec)\n",
    "    \n",
    "    # Pillow is a fork of PIL (standard Python image library), we consider Pillow.Image to be regular Python images\n",
    "    # In order to use Pillow features like crop, we have to convert the image from numpy into PIL (Pillow)\n",
    "    pillow_image = Pillow.Image.fromarray(colored_np) # convert to regular python image\n",
    "    \n",
    "    # When you manually convert a spectrogram to an image without using matplotlib you have to flip it vertically\n",
    "    flipped_image = pillow_image.transpose(Image.FLIP_TOP_BOTTOM) # flip image\n",
    "    \n",
    "    # This will crop the image by taking from the height to make a square\n",
    "    h, w, *other = pillow_image.shape\n",
    "    cropped_image = flipped_image.crop((0, h-w, w, h))\n",
    "    cropped_image_np = np.array(cropped_image) # back to numpy\n",
    "        \n",
    "    # Kinda confusing but fast.ai has a class called PILImage and so we convert our real PIL image into a fast.ai one\n",
    "    fast_ai_image = PILImage.create(cropped_image_np)\n",
    "    return fast_ai_image\n",
    "\n",
    "\n",
    "# I chose to load the audio files directly into fast.ai using the DataBlock API.\n",
    "# Alternatively, we could have pre-computed all the spectrograms in the 00_getting_started.ipynb but\n",
    "# I decided against it because then it would be unlikely for anyone to actually make modifications to the images\n",
    "# Writing 40,000 files to disk is painfully slow so you wouldn't even be able to get started quickly\n",
    "# This method creates a transformer which can take paths to audio files and transform them into spectrogram images.\n",
    "# If you do some research you will see lots of people doing pretty terrible things involving file.io because\n",
    "# it is not easy to figure out how to turn a spectrogram into a 3-channel image and further how to get that\n",
    "# into a fast.ai image. This solution is pretty clever because since it is all in-memory it is insanely fast and\n",
    "# does not rely on any type of pre-computation.\n",
    "class SpecgramTransform(Transform):\n",
    "    def __init__(self): self.aug = create_spectrogram\n",
    "    def encodes(self, audio_file: Path):\n",
    "        aug_img, sample_rate = self.aug(audio_file)\n",
    "        image = spectrogram_to_image(aug_img)\n",
    "        return image\n",
    "\n",
    "# Alternative transformer for generating images of Mel Spectrograms\n",
    "class MelSpecgramTransform(Transform):\n",
    "    def __init__(self): self.aug = create_mel_spectrogram\n",
    "    def encodes(self, audio_file: Path):\n",
    "        aug_img, sample_rate = self.aug(audio_file)\n",
    "        image = spectrogram_to_image(aug_img)\n",
    "        return image\n",
    "\n",
    "    \n",
    "# IGNORE: Unless you decide to use SpecgramTransform \n",
    "# We may need to crop images. If you use the SpecgramTransform, cropping will likely be required \n",
    "# to turn the image into a square.\n",
    "# It could be reasonable to cut-off the image at frequencies we know a whale call couldn't exist\n",
    "# Ultimately, images will need to be square I believe so they would need to get filled with something\n",
    "# The reason I created this transform is because fast.ai doesn't give you a transformer for precision cropping\n",
    "# You either crop and cut out the center or you don't crop at all\n",
    "# This transformer lets you optionally crop from any direction and leaves sides alone that you don't specify crops for\n",
    "class CropImageTransform(Transform):\n",
    "    def __init__(self, left=None, upper=None, right=None, lower=None):\n",
    "        print(\"test constructor\")\n",
    "        self.aug = self.__crop_image\n",
    "        self.box = (left, upper, right, lower)\n",
    "    \n",
    "    def __compute_box(self, image):\n",
    "        # get current dimensions of image\n",
    "        # *other is because we don't know if we will receive two elements or more\n",
    "        # we get two for a greyscale image, we get three for a RGB\n",
    "        h, w, *other = image.shape\n",
    "        \n",
    "        #get desired crop entered by user\n",
    "        left, upper, right, lower = self.box\n",
    "        \n",
    "        #don't crop sides that user didn't want cropped\n",
    "        left = 0 if left is None else left\n",
    "        upper = 0 if upper is None else upper\n",
    "        right = w if right is None else right\n",
    "        lower = h if lower is None else lower\n",
    "        \n",
    "        #save computed box\n",
    "        self.box = (left, upper, right, lower)\n",
    "        \n",
    "    def __crop_image(self, image):\n",
    "        print(\"test crop\")\n",
    "\n",
    "        self.__compute_box(image)\n",
    "        print(self.box)\n",
    "        image_cropped = image.crop(self.box)\n",
    "        return image_cropped\n",
    "        \n",
    "    # this transformer works on PILImages (fast.ai)\n",
    "    # this transfomer does not work on PIL.Image (Pillow/PIL)\n",
    "    def encodes(self, image):\n",
    "        print(\"encode\")\n",
    "        print(image)\n",
    "        cropped_image = self.aug(image)\n",
    "        return cropped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check one file to see if data loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(path)\n",
    "audio_file = files[0]\n",
    "spec, sample_rate = create_mel_spectrogram(audio_file)\n",
    "spec_image = spectrogram_to_image(spec)\n",
    "print(type(spec_image))\n",
    "# show_image(spec_image)\n",
    "\n",
    "print(f\"Specgram Size: {spec_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 1: _Describe the idea_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write a brief description of what you are going to try differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and transform data\n",
    "dblock = DataBlock(\n",
    "    blocks    = (TransformBlock, CategoryBlock),\n",
    "    get_items = get_files,\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    get_y=parent_label,\n",
    "    item_tfms = [ SpecgramTransform() ]\n",
    ")\n",
    "dls = dblock.dataloaders(path)\n",
    "\n",
    "# train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation 2: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Write up a summary of the results you achieved. Why do you suspect that certain things worked well? Why do you suspect other things didn't work so well? What did you learn? What surprised you? What, if any, would be additional next steps? What new questions do you have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
