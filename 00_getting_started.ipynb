{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help install all the required dependencies as well as prepare the dataset for use with fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python version\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is needed for systems that don't already have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y libsndfile1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up system dependencies that work well with each other is incredibly difficult. Different libraries specify different version constraints and that makes finding the right versions difficult to resolve. In the cloud, typically libraries like `torch` will already be installed on the system with CUDA enabled. When a dependency specifies a different version of `torch` and your system downloads a new one, it may not be CUDA enabled. The same occurs with other libraries.\n",
    "\n",
    "`conda` ships with the Anaconda distribution of Python and should be used instead of `pip` as certain channels curate which libraries work best together. `fastchan` is a channel created by the fast.ai team that curates the most common versions data science packages that work well together and also have GPU acceleration. When a package isn't on `fastchan` we use `conda-forge` which is a community maintained channel that is a good alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install fastai\n",
    "!conda install -c fastchan fastai torchaudio --yes\n",
    "!conda install -c conda-forge kaggle librosa --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Kaggle’s public API, you must first authenticate using an API token. From the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings at https://www.kaggle.com/account. Scroll down to the section of the page labelled API:\n",
    "\n",
    "To create a new token, click on the “Create New API Token” button. This will download a fresh authentication token onto your machine.\n",
    "\n",
    "### Accept the rules\n",
    "\n",
    "https://www.kaggle.com/competitions/whale-detection-challenge/rules\n",
    "\n",
    "\n",
    "### Upload your `kaggle.json` to the same folder as this notebook then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle; mv kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c whale-detection-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean up our workspace so we can start over if necessary but also setup the directories we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf full_data; rm -rf sample_data; rm -rf tmp_data; rm -rf full_image_data; #remove any existing extracted data\n",
    "!unzip -q whale-detection-challenge.zip -d data/ #unzip main file\n",
    "!unzip -q data/small_data_sample_revised.zip -d sample_data/ #unzip sample data\n",
    "!unzip -q data/whale_data.zip -d tmp_data/ #unzip full data\n",
    "!rm -rf data/; rm -rf tmp_data/data/test; #remove unneeded files. official test data isn't used because we don't have labels\n",
    "!mkdir full_data; mv tmp_data/data/train full_data/audio; #move stuff around\n",
    "!mv tmp_data/data/train.csv full_data/labels.csv #rename labels\n",
    "!rm -rf tmp_data #remove tmp directory\n",
    "!mkdir -p full_data/whale; mkdir -p full_data/not_whale; #create necessary folders\n",
    "!mkdir -p full_image_data/whale; mkdir -p full_image_data/not_whale; #create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "import pandas as pd\n",
    "import os;\n",
    "import shutil;\n",
    "\n",
    "DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'full_data'))\n",
    "DATA_META_FILE=os.path.join(DATA_ROOT_DIR, 'labels.csv')\n",
    "DATA_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'audio')\n",
    "DATA_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'whale')\n",
    "DATA_NOT_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'not_whale')\n",
    "\n",
    "df = pd.read_csv(DATA_META_FILE)\n",
    "df.head()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clip_name = row['clip_name']\n",
    "    label = row['label']\n",
    "    \n",
    "    # path to file described in labels.csv\n",
    "    source_path = os.path.join(DATA_AUDIO_DIR, clip_name)\n",
    "\n",
    "    # this is the directory we will move it to\n",
    "    aiff_dst_path = None\n",
    "    \n",
    "    # path will be dependent on whether it is a whale sound or not\n",
    "    if(label==0): #not whale\n",
    "        aiff_dst_path = os.path.join(DATA_NOT_WHALE_AUDIO_DIR, clip_name)\n",
    "    else: #whale\n",
    "        aiff_dst_path = os.path.join(DATA_WHALE_AUDIO_DIR, clip_name)\n",
    "\n",
    "    # perform the move, this is pretty fast\n",
    "    shutil.move(source_path, aiff_dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it moved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_META_FILE)\n",
    "df.head()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clip_name = row['clip_name']\n",
    "    label = row['label']\n",
    "    source_path = os.path.join(DATA_AUDIO_DIR, clip_name)\n",
    "    not_whale_destination_path = os.path.join(DATA_NOT_WHALE_AUDIO_DIR, clip_name)\n",
    "    whale_destination_path = os.path.join(DATA_WHALE_AUDIO_DIR, clip_name)\n",
    "\n",
    "    if(label==0): #not whale\n",
    "        assert(os.path.exists(not_whale_destination_path)), f\"{clip_name} should be in {not_whale_destination_path}\"\n",
    "        assert not(os.path.exists(whale_destination_path)), f\"{clip_name} should not be in {whale_destination_path}\"\n",
    "    else: #whale\n",
    "        assert not(os.path.exists(not_whale_destination_path)), f\"{clip_name} should not be in {not_whale_destination_path}\"\n",
    "        assert(os.path.exists(whale_destination_path)), f\"{clip_name} should be in {whale_destination_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `labels.csv` since we no longer need it since files are organized in folders according to their labell.\n",
    "If you don't do this you will run into a dataloader issue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf full_data/audio; rm -rf full_data/labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
