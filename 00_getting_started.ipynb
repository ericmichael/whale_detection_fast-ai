{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will help install all the required dependencies as well as prepare the dataset for use with fast.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python version\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is needed for systems that don't already have it installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y libsndfile1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up system dependencies that work well with each other is incredibly difficult. Different libraries specify different version constraints and that makes finding the right versions difficult to resolve. In the cloud, typically libraries like `torch` will already be installed on the system with CUDA enabled. When a dependency specifies a different version of `torch` and your system downloads a new one, it may not be CUDA enabled. The same occurs with other libraries.\n",
    "\n",
    "`conda` ships with the Anaconda distribution of Python and should be used instead of `pip` as certain channels curate which libraries work best together. `fastchan` is a channel created by the fast.ai team that curates the most common versions data science packages that work well together and also have GPU acceleration. When a package isn't on `fastchan` we use `conda-forge` which is a community maintained channel that is a good alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - fastai\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  python_abi         fastchan/linux-64::python_abi-3.7-2_cp37m None\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main/linux-64::certifi-2022.9.24~ --> fastchan/noarch::certifi-2022.9.24-pyhd8ed1ab_0 None\n",
      "  conda              pkgs/main::conda-22.9.0-py37h06a4308_0 --> fastchan::conda-22.9.0-py37h89c1867_0 None\n",
      "\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/conda\n",
      "\n",
      "  added / updated specs:\n",
      "    - kaggle\n",
      "    - librosa\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    certifi-2022.9.24          |     pyhd8ed1ab_0         155 KB  conda-forge\n",
      "    gettext-0.19.8.1           |    h0b5b191_1005         3.6 MB  conda-forge\n",
      "    libflac-1.3.4              |       h27087fc_0         474 KB  conda-forge\n",
      "    libllvm11-11.1.0           |       hf817b99_2        29.1 MB  conda-forge\n",
      "    libsndfile-1.0.31          |       h9c3ff4c_1         602 KB  conda-forge\n",
      "    llvmlite-0.38.0            |   py37h4ff587b_0         2.1 MB\n",
      "    numba-0.55.1               |   py37h51133e4_0         3.4 MB\n",
      "    tbb-2021.6.0               |       hdb19cb5_0         1.4 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        40.8 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  appdirs            conda-forge/noarch::appdirs-1.4.4-pyh9f0ad1d_0 None\n",
      "  audioread          conda-forge/linux-64::audioread-3.0.0-py37h89c1867_0 None\n",
      "  gettext            conda-forge/linux-64::gettext-0.19.8.1-h0b5b191_1005 None\n",
      "  libflac            conda-forge/linux-64::libflac-1.3.4-h27087fc_0 None\n",
      "  libllvm11          conda-forge/linux-64::libllvm11-11.1.0-hf817b99_2 None\n",
      "  libogg             conda-forge/linux-64::libogg-1.3.4-h7f98852_1 None\n",
      "  librosa            conda-forge/noarch::librosa-0.9.2-pyhd8ed1ab_0 None\n",
      "  libsndfile         conda-forge/linux-64::libsndfile-1.0.31-h9c3ff4c_1 None\n",
      "  libvorbis          conda-forge/linux-64::libvorbis-1.3.7-h9c3ff4c_0 None\n",
      "  llvmlite           pkgs/main/linux-64::llvmlite-0.38.0-py37h4ff587b_0 None\n",
      "  numba              pkgs/main/linux-64::numba-0.55.1-py37h51133e4_0 None\n",
      "  pooch              conda-forge/noarch::pooch-1.6.0-pyhd8ed1ab_0 None\n",
      "  pysoundfile        conda-forge/noarch::pysoundfile-0.11.0-pyhd8ed1ab_0 None\n",
      "  resampy            conda-forge/noarch::resampy-0.4.2-pyhd8ed1ab_0 None\n",
      "  tbb                pkgs/main/linux-64::tbb-2021.6.0-hdb19cb5_0 None\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2022.07.19~ --> conda-forge::ca-certificates-2022.9.24-ha878542_0 None\n",
      "  conda               fastchan::conda-22.9.0-py37h89c1867_0 --> conda-forge::conda-22.9.0-py37h89c1867_1 None\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi                                          fastchan --> conda-forge None\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "libllvm11-11.1.0     | 29.1 MB   | ##################################### | 100% \n",
      "llvmlite-0.38.0      | 2.1 MB    | ##################################### | 100% \n",
      "gettext-0.19.8.1     | 3.6 MB    | ##################################### | 100% \n",
      "tbb-2021.6.0         | 1.4 MB    | ##################################### | 100% \n",
      "numba-0.55.1         | 3.4 MB    | ##################################### | 100% \n",
      "libflac-1.3.4        | 474 KB    | ##################################### | 100% \n",
      "certifi-2022.9.24    | 155 KB    | ##################################### | 100% \n",
      "libsndfile-1.0.31    | 602 KB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "# Install fastai\n",
    "!conda install -c fastchan fastai --yes\n",
    "!conda install -c conda-forge kaggle librosa --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Kaggle’s public API, you must first authenticate using an API token. From the site header, click on your user profile picture, then on “My Account” from the dropdown menu. This will take you to your account settings at https://www.kaggle.com/account. Scroll down to the section of the page labelled API:\n",
    "\n",
    "To create a new token, click on the “Create New API Token” button. This will download a fresh authentication token onto your machine.\n",
    "\n",
    "### Accept the rules\n",
    "\n",
    "https://www.kaggle.com/competitions/whale-detection-challenge/rules\n",
    "\n",
    "\n",
    "### Upload your `kaggle.json` to the same folder as this notebook then run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle; mv kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c whale-detection-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clean up our workspace so we can start over if necessary but also setup the directories we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf full_data; rm -rf sample_data; rm -rf tmp_data; rm -rf full_image_data; #remove any existing extracted data\n",
    "!unzip -q whale-detection-challenge.zip -d data/ #unzip main file\n",
    "!unzip -q data/small_data_sample_revised.zip -d sample_data/ #unzip sample data\n",
    "!unzip -q data/whale_data.zip -d tmp_data/ #unzip full data\n",
    "!rm -rf data/; rm -rf tmp_data/data/test; #remove unneeded files. official test data isn't used because we don't have labels\n",
    "!mkdir full_data; mv tmp_data/data/train full_data/audio; #move stuff around\n",
    "!mv tmp_data/data/train.csv full_data/labels.csv #rename labels\n",
    "!rm -rf tmp_data #remove tmp directory\n",
    "!mkdir -p full_data/whale; mkdir -p full_data/not_whale; #create necessary folders\n",
    "!mkdir -p full_image_data/whale; mkdir -p full_image_data/not_whale; #create necessary folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "import pandas as pd\n",
    "import os;\n",
    "\n",
    "DATA_ROOT_DIR=os.path.normpath(os.path.join(os.getcwd(), 'full_data'))\n",
    "DATA_META_FILE=os.path.join(DATA_ROOT_DIR, 'labels.csv')\n",
    "DATA_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'audio')\n",
    "DATA_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'whale')\n",
    "DATA_NOT_WHALE_AUDIO_DIR=os.path.join(DATA_ROOT_DIR, 'not_whale')\n",
    "\n",
    "df = pd.read_csv(DATA_META_FILE)\n",
    "df.head()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clip_name = row['clip_name']\n",
    "    label = row['label']\n",
    "    \n",
    "    # path to file described in labels.csv\n",
    "    source_path = os.path.join(DATA_AUDIO_DIR, clip_name)\n",
    "\n",
    "    # this is the directory we will move it to\n",
    "    aiff_dst_path = None\n",
    "    \n",
    "    # path will be dependent on whether it is a whale sound or not\n",
    "    if(label==0): #not whale\n",
    "        aiff_dst_path = os.path.join(DATA_NOT_WHALE_AUDIO_DIR, clip_name)\n",
    "    else: #whale\n",
    "        aiff_dst_path = os.path.join(DATA_WHALE_AUDIO_DIR, clip_name)\n",
    "\n",
    "    # perform the move, this is pretty fast\n",
    "    shutil.move(source_path, aiff_dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that it moved correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_META_FILE)\n",
    "df.head()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    clip_name = row['clip_name']\n",
    "    label = row['label']\n",
    "    source_path = os.path.join(DATA_AUDIO_DIR, clip_name)\n",
    "    not_whale_destination_path = os.path.join(DATA_NOT_WHALE_AUDIO_DIR, clip_name)\n",
    "    whale_destination_path = os.path.join(DATA_WHALE_AUDIO_DIR, clip_name)\n",
    "\n",
    "    if(label==0): #not whale\n",
    "        assert(os.path.exists(not_whale_destination_path)), f\"{clip_name} should be in {not_whale_destination_path}\"\n",
    "        assert not(os.path.exists(whale_destination_path)), \"f{clip_name} should not be in {whale_destination_path}\"\n",
    "    else: #whale\n",
    "        assert not(os.path.exists(not_whale_destination_path)), f\"{clip_name} should not be in {not_whale_destination_path}\"\n",
    "        assert(os.path.exists(whale_destination_path)), f\"{clip_name} should be in {whale_destination_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove `labels.csv` since we no longer need it since files are organized in folders according to their labell.\n",
    "If you don't do this you will run into a dataloader issue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf full_data/audio; rm -rf full_data/labels.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
